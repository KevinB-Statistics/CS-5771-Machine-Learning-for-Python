{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyycgYG_tRJe"
      },
      "source": [
        "## Python For Machine Learning Fall 2025\n",
        "---\n",
        "# Univariate Linear Regression (in Numpy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT7rh_sDf2Bg"
      },
      "source": [
        "Univariate Linear Regression, or Simple Linear Regression, is the simplest form of Linear Regression. The model predict one target using one variable. Thus, each sample $x_i$, is a scalar. Each $x_i$ associates with one target $y_i$. The linear model is\n",
        "$$\n",
        "y = wx + b\n",
        "$$\n",
        "where $w$ is the coefficent and the $b$ is intercept. The problem is given a series of samples $x_1, x_2,..., x_n$, and a series of targets $y_1, y_2,..., y_n$. Our goal is to find the best $w$ and $b$, that can accurate predict the value $y$ when given $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWRbDq-mhJL1"
      },
      "source": [
        "As we proposed that we use the following target function, or loss function:\\\n",
        "\\begin{equation}\n",
        "\\Sigma^n_{i=1}(\\hat{y_i}-y_i)\n",
        "\\end{equation}\n",
        "where we have:\\\n",
        "$$\n",
        "\\hat{y_i}=wx_i +b\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3mgfoo5jHZW"
      },
      "source": [
        "The problem of the above optimization objective is that the result of $\\hat{y_i}-y_i$ could be positive and also negative, which will cancel each other out when summing up. We need to either use the absolute value or use the squared values. This lead to the sum of squared errors (SSE) loss:\n",
        "$$\n",
        "\\Sigma^n_{i=1}(\\hat{y_i}-y_i)^2\n",
        "$$\n",
        "\n",
        "Substitute that $\\hat{y_i}=wx_i+b$, we can write the loss functions as:\n",
        "$$\n",
        "L(w,b)=\\Sigma^n_{i=1}(wx_i+b-y_i)^2\n",
        "$$\n",
        "\n",
        "The final goal is that to find the optimized $w$ and $b$ that minimize the loss, which is:\n",
        "$$\n",
        "argmin_{w,b}\\Sigma^n_{i=1}(wx_i+b-y_i)^2\n",
        "$$\n",
        "The good news is that the optimization of this function has analytical solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mo0wr8_UsonB"
      },
      "source": [
        "We can find the optimized $w$ and $b$ by solving the equation that the derivate of SSE loss to zero. Now let's calculate the partial derivates of SSE regarding $w$ and $b$, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-shfCyyrenWo"
      },
      "source": [
        "Among the two, the partial derivate regarding $b$ is simplier. We can calculate $b$ as follows:\n",
        "$$\n",
        "\\frac{\\partial L(w,b)}{\\partial b}=\\Sigma^n_{i=1}2(wx_i+b-y_i)(wx_i+b-y_i)_b'\n",
        "$$\n",
        "\n",
        "which is\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(w,b)}{\\partial b}=2\\Sigma^n_{i=1}(wx_i+b-y_i)\n",
        "$$\n",
        "\n",
        "Set the partial derivate equal to $0$, we get equation:\n",
        "$$\n",
        "\\Sigma^n_{i=1}(wx_i+b-y_i) = 0\n",
        "$$\n",
        "\n",
        "Write the equation out, we get\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i+\\Sigma^n_{i=1}b -\\Sigma^n_{i=1}y_i=0\n",
        "$$\n",
        "\n",
        "Since $\\Sigma^n_{i=1}b=nb$, we have\n",
        "$$\n",
        "b = \\frac{\\Sigma^n_{i=1}y_i - w\\Sigma^n_{i=1}x_i}{n}\n",
        "$$\n",
        "\n",
        "The equation contains another variable $w$, and we can not solve the equation right now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjpQku1nehjL"
      },
      "source": [
        "\n",
        "Next step, we calculate the partial derivate regarding $w$ as follows:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(w,b)}{\\partial b}=\\Sigma^n_{i=1}2(wx_i+b-y_i)(wx_i+b-y_i)_w'\n",
        "$$\n",
        "\n",
        "Which is,\n",
        "\n",
        "$$\n",
        "\\frac{\\partial L(w,b)}{\\partial b}=\\Sigma^n_{i=1}2(wx_i+b-y_i)x_i\n",
        "$$\n",
        "\n",
        "Write it out, we get\n",
        "$$\n",
        "\\frac{\\partial L(w,b)}{\\partial b}=2\\Sigma^n_{i=1}wx_i^2 + 2b\\Sigma^n_{i=1}x_i  - 2\\Sigma^n_{i=1}x_iy_i\n",
        "$$\n",
        "\n",
        "Set the partial derivate to $0$, we have:\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i^2+b\\Sigma^n_{i=1}x_i-\\Sigma^n_{i=1}x_iy_i=0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r787mBDIjCwN"
      },
      "source": [
        "Now we substitute\n",
        "$$\n",
        "b = \\frac{\\Sigma^n_{i=1}y_i - w\\Sigma^n_{i=1}x_i}{n}\n",
        "$$\n",
        "into\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i^2+b\\Sigma^n_{i=1}x_i-\\Sigma^n_{i=1}x_iy_i=0\n",
        "$$\n",
        "\n",
        "We get\n",
        "\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i^2+\\frac{\\Sigma^n_{i=1}y_i - w\\Sigma^n_{i=1}x_i}{n}\\Sigma^n_{i=1}x_i-\\Sigma^n_{i=1}x_iy_i=0\n",
        "$$\n",
        "\n",
        "Write the multiuplication out\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i^2+\\frac{\\Sigma^n_{i=1}y_i - w\\Sigma^n_{i=1}x_i}{n}\\Sigma^n_{i=1}x_i-\\Sigma^n_{i=1}x_iy_i=0\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\Sigma^n_{i=1}wx_i^2 + \\frac{1}{n}\\Sigma^n_{i=1}y_i \\Sigma^n_{i=1}x_i - \\frac{1}{n}w\\Sigma^n_{i=1}x_i \\Sigma^n_{i=1}x_i - \\Sigma^n_{i=1}x_iy_i=0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1dgGtLHl10W"
      },
      "source": [
        "Using transposition,\n",
        "$$\n",
        "w\\Sigma^n_{i=1}x_i^2-w\\frac{1}{n}\\Sigma^n_{i=1}x_i \\Sigma^n_{i=1}x_i =\\Sigma^n_{i=1}x_iy_i-\\frac{1}{n}\\Sigma^n_{i=1}y_i \\Sigma^n_{i=1}x_i\n",
        "$$\n",
        "\n",
        "Note $\\bar{x}=\\frac{1}{n}\\Sigma^n_{i=1}x_i$, and $\\bar{y}=\\frac{1}{n}\\Sigma^n_{i=1}y_i$, we get a simpler equation:\n",
        "$$\n",
        "w=\\frac{\\Sigma^n_{i=1}x_iy_i-\\Sigma^n_{i=1}y_i\\bar{x}}{\\Sigma^n_{i=1}x_i^2-\\Sigma^n_{i=1}x_i\\bar{x}}\n",
        "$$\n",
        "\n",
        "which is\n",
        "$$\n",
        "w=\\frac{\\Sigma^n_{i=1}y_i(x_i-\\bar{x})}{\\Sigma^n_{i=1}x_i(x_i-\\bar{x})}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--uAYo7eP_k_"
      },
      "source": [
        "The numerator and denominator can be expressed in terms of convariance an variance, to provide more stable and convinient calculation. Let's start from the first form of our equation:\n",
        "$$\n",
        "w=\\frac{\\Sigma^n_{i=1}x_iy_i-\\Sigma^n_{i=1}y_i\\bar{x}}{\\Sigma^n_{i=1}x_i^2-\\Sigma^n_{i=1}x_i\\bar{x}}\n",
        "$$\n",
        "\n",
        "Since we have:\n",
        "$$\n",
        "n\\bar{x}\\bar{y}=n\\bar{x}\\frac{1}{n}\\Sigma_{i=1}^ny_i=\\Sigma_{i=1}^ny_i\\bar{x}\n",
        "$$\n",
        ",\n",
        "$$\n",
        "n\\bar{x}\\bar{y}=n\\bar{y}\\frac{1}{n}\\Sigma_{i=1}^nx_i=\\Sigma_{i=1}^nx_i\\bar{y}\n",
        "$$\n",
        ", and\n",
        "$$\n",
        "n\\bar{x}\\bar{y}=\\Sigma_{i=1}\\bar{x}\\bar{y}\n",
        "$$\n",
        "\n",
        "We can rewrite the numerator as:\n",
        "$$\n",
        "\\Sigma^n_{i=1}x_iy_i-\\Sigma^n_{i=1}y_i\\bar{x}=\\Sigma^n_{i=1}(x_iy_i-x_i\\bar{y}-y_i\\bar{x}+\\bar{x}\\bar{y})\n",
        "$$\n",
        "Futhur, it can be rewrote as:\n",
        "$$\n",
        "\\Sigma^n_{i=1}(x_i(y_i-\\bar{y})-\\bar{x}(y_i-\\bar{y}))=\\Sigma^n_{i=1}((y_i-\\bar{y})(x_i-\\bar{x}))\n",
        "$$\n",
        "Whic is the covariance between variable $x$ and $y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou8dhMgTdRls"
      },
      "source": [
        "For denominator, we can rewrite it as:\n",
        "$$\n",
        "\\Sigma^n_{i=1}x_i^2-\\Sigma^n_{i=1}x_i\\bar{x}=\\Sigma^n_{i=1}(x_i^2-x_i\\bar{x})\n",
        "$$\n",
        "\n",
        "We can get the same conclusion that:\n",
        "$$\n",
        "n\\bar{x}\\bar{x}=\\Sigma_{i=1}^n\\bar{x}\\bar{x}=\\Sigma_{i=1}^nx_i\\bar{x}=\\Sigma_{i=1}^n\\bar{x}x_i\n",
        "$$\n",
        "So, the denominator equals to\n",
        "$$\n",
        "\\Sigma^n_{i=1}(x_i^2-x_i\\bar{x})=\\Sigma^n_{i=1}(x_i^2-x_i\\bar{x}-x_i\\bar{x}+\\bar{x}\\bar{x}) = \\Sigma_{i-1}^n(x_i-\\bar{x})^2\n",
        "$$\n",
        "\n",
        "Which is the variance of $x$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl7IdRW8PcAA"
      },
      "source": [
        "Thus, we get a stable and computable formula of $w$, as follows:\n",
        "$$\n",
        "w=\\frac{\\Sigma^n_{i=1}((y_i-\\bar{y})(x_i-\\bar{x}))}{\\Sigma_{i-1}^n(x_i-\\bar{x})^2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXdGW6CKoMaI"
      },
      "source": [
        "Given the value of $w$ in hand, we can calculate the value of $b$ accordingly. Recall the equation of $b$, as follows:\n",
        "\n",
        "$$\n",
        "b = \\frac{\\Sigma^n_{i=1}y_i - w\\Sigma^n_{i=1}x_i}{n}\n",
        "$$\n",
        "\n",
        "We can rewrite it as\n",
        "\n",
        "$$\n",
        "b = \\bar{y}-w\\bar{x}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LRJJjsw-70C"
      },
      "source": [
        "We mentioned that `sklearn` is based one a few packages. One of the most import libary is `numpy`. Now let's implement the univariate linear regression in numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DmnlXHLo_z_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghqDECHg_YDa"
      },
      "source": [
        "Use `make_regression` to randomly generate a group of `samples`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fhhpoZkn_7F0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100, 1) (100,) 41.7411003148779\n"
          ]
        }
      ],
      "source": [
        "feature, target, coef = make_regression(n_samples=100, n_features=1, random_state=42, noise=10, coef=True, bias=5)\n",
        "print(feature.shape, target.shape, coef)\n",
        "#print(type(feature), feature)\n",
        "#print(type(target),target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI60jQ8f_3QH"
      },
      "source": [
        "It is always easier if you can visualize the data. That is also another benefits we use simple linear regression as example, in addition to the simplicity of the math.\n",
        "\n",
        "Here we use `matplotlib` library to do the visualization. The `scatter` function can conviniently plot the sample points on a 2d surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Od_wAWiBr2I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(feature, target)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYuvN3OgCEYM"
      },
      "source": [
        "Now let's calculate the value of $w$, according to our equation:\n",
        "$$\n",
        "w=\\frac{\\Sigma^n_{i=1}((y_i-\\bar{y})(x_i-\\bar{x}))}{\\Sigma_{i-1}^n(x_i-\\bar{x})^2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcDC_2oZCj1_"
      },
      "source": [
        "The features and targets are already in `numpy` array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NfgrrYLCrZL"
      },
      "outputs": [],
      "source": [
        "print(type(feature), type(target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPGLz2RAC2yW"
      },
      "source": [
        "In our case, `feature` and `target` are slightly different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdUkov5sC7C-"
      },
      "outputs": [],
      "source": [
        "print(feature.shape, target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03l23r4ZDOoR"
      },
      "source": [
        "`feature` is a 2d array, and `target` is a 1d array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHLMZrxGDVik"
      },
      "outputs": [],
      "source": [
        "print(feature.ndim, target.ndim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6UiTU6RCQP7"
      },
      "source": [
        "Task 1 will be calculate the mean value of $\\bar{x}$ and $\\bar{y}$. `numpy` provide `mean` function to calculate the mean value of an array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Y9ed5WACxx"
      },
      "outputs": [],
      "source": [
        "y_bar = np.mean(target)\n",
        "x_bar = np.mean(feature)\n",
        "print(y_bar, x_bar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQMvloDrD7zy"
      },
      "source": [
        "`numpy` array support a cool feature, which is called `broadcast`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNym6uujAetn"
      },
      "outputs": [],
      "source": [
        "y_sub = target - y_bar\n",
        "x_sub = feature - x_bar\n",
        "print(y_sub.shape, x_sub.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3iWF8dYEP_j"
      },
      "source": [
        "Now we need to do a point-to-point, or element-wise mulitplication in between array `feature` and `y_sub`, as well as `feature` and `x_sub`. One issue is that due to the different `ndim` values between the two, the multiplication will perform wierd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VwOV2Q0FRja"
      },
      "outputs": [],
      "source": [
        "nominator = np.sum(y_sub*x_sub.flatten())\n",
        "denominator = np.sum(x_sub.flatten()*x_sub.flatten())\n",
        "w = nominator/denominator\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uY4gwCqFYte"
      },
      "source": [
        "No we can calculate the value of $b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIrmBJM7FcgX"
      },
      "outputs": [],
      "source": [
        "b = y_bar - w*x_bar\n",
        "print(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ktpfPtPFvqs"
      },
      "source": [
        "Now we have the model $y=wx+b$ estimated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmAi7_a9F3ui"
      },
      "source": [
        "To test it, we create a function that create the output based on $w$ and $b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxokuTeQF_mN"
      },
      "outputs": [],
      "source": [
        "def predict(x, w, b):\n",
        "    return w*x+b\n",
        "\n",
        "preds = predict(feature, w, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgMyfbhNGKw1"
      },
      "source": [
        "Now we are ready to plot the data and the `line` we estimated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqlew6eoGE0Z"
      },
      "outputs": [],
      "source": [
        "plt.scatter(feature, target)\n",
        "plt.plot(feature, preds, color='red')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeZmk_nLhpyN"
      },
      "source": [
        "Validate with `sklearn`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ag1Pk3yjnmL"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression().fit(feature, target)\n",
        "print(f'our estimation w: {w} sklearn estimation: {model.coef_},our estimation b: {b} sklearn estimation:{model.intercept_}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
