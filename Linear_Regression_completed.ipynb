{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVfx1GnHr6Hr"
      },
      "source": [
        "## Python For Machine Learning Fall 2025\n",
        "---\n",
        "# Linear Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3OkaCLpkRV1"
      },
      "source": [
        "\n",
        "### 1. Introduction\n",
        "Linear regression is one of the simplest supervised learning algorithms. In fact, it is so simple that it is sometimes not considered machine learning at all! Whatever you believe, the fact is that linear regression--and its extensions--continues to be a common and useful method of making predictions when the target vector is a quantitative value (e.g. home price, age)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2nWMpZ6tbMw"
      },
      "source": [
        "#### 1.1 Early Concepts and the Method of Least Squares\n",
        "\n",
        "The true birth of linear regression is tied to the development of the method of least squares. While several brilliant minds worked on similar problems, two figures are most prominently credited with its creation in the early 19th century.\n",
        "\n",
        "- Adrien-Marie Legendre: the first to publish the method of least squares in 1805\n",
        "- Carl Friedrich Gauss: Gauss claimed he had been using the method since 1795 but didn't publish it until 1809\n",
        "\n",
        "#### 1.2 Regression\n",
        "\n",
        "In the 1880s, Sir Francis Galton was studying the relationship between the heights of parents and their children. He observed that very tall parents tended to have children who were tall, but slightly shorter than them. Similarly, very short parents tended to have children who were short, but slightly taller than them.\n",
        "\n",
        "He called this phenomenon \"regression towards mediocrity,\" which was later termed \"regression to the mean.\" He developed a graphical method to describe this relationship, plotting the parents' heights against the children's heights and drawing a line through the data. He used the term \"regression line\" to describe this relationship. This is where the name of the technique comes from, even though today its primary use is for prediction, not just describing this specific biological phenomenon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrPedJx2txfG"
      },
      "source": [
        "#### 1.3 Problem Statement\n",
        "You want to train a model that represents a linear relationship between the feature and target vector.\n",
        "\n",
        "#### 1.4 Solution\n",
        "Use a linear regression (`LinearRegression` in scikit-learn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "2mFSHAo0r6Hs"
      },
      "outputs": [],
      "source": [
        "# load libraries\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# generate feature matrix, target vector\n",
        "features, target = make_regression(n_samples=100, n_features=3, n_informative=3, random_state=1)\n",
        "\n",
        "# create linear regression\n",
        "regression = LinearRegression()\n",
        "\n",
        "# fit the linear regression\n",
        "model = regression.fit(features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyG9mLGsv88z"
      },
      "source": [
        "#### 1.5 Code Interpretation\n",
        "\n",
        "The script uses `scikit-learn` library to perform three main steps:\n",
        "\n",
        "- Generate a synthetic dataset suitable for a regression problem.\n",
        "- Create an instance of a linear regression model.\n",
        "- Train (or \"fit\") the model to the generated data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW5tCOTSxkNN"
      },
      "source": [
        "##### 1.5.1 Generating Data\n",
        "`features, target = make_regression(n_samples=100, n_features=3, n_informative=3, random_state=1)`\n",
        "\n",
        "This line calls the make_regression function to create our dataset.\n",
        "\n",
        "`n_samples=100`: This specifies that we want `100` observations or rows in our dataset.\n",
        "\n",
        "`n_features=3`: This means each sample will have `3` descriptive features (or columns).\n",
        "\n",
        "`n_informative=3`: This tells the function that all `3` features should be \"informative,\" meaning they all have a meaningful influence on the target value.\n",
        "\n",
        "`random_state=1`: This is a seed for the random number generator. Using a specific random_state ensures that every time this code is run, the exact same \"random\" data is generated. This is crucial for getting reproducible results.\n",
        "\n",
        "The function returns two things:\n",
        "\n",
        "`features`: A matrix (like a table) with `100` rows and `3` columns. These are the independent variables.\n",
        "\n",
        "`target`: A vector (a single column) with `100` values. This is the dependent variable that the model will learn to predict."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX6aSRrtxlSj"
      },
      "source": [
        "##### 1.5.2 Creating the Model\n",
        "`regression = LinearRegression()`\n",
        "\n",
        "This line creates an instance of the LinearRegression model. You can think of regression as a blank, untrained model object, ready to learn from data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aSW_KFqx46H"
      },
      "source": [
        "##### 1.5.3 Training the model\n",
        "\n",
        "`model = regression.fit(features, target)`\n",
        "\n",
        "This is the most critical step. The `.fit()` method is where the model training happens.\n",
        "\n",
        "`regression.fit(features, target)`: The features and target data were passed to the fit method. The linear regression algorithm then analyzes this data to find the optimal coefficients (or weights) for the three features that best predict the target value. It essentially \"learns\" the mathematical relationship between the inputs (`features`) and the output (`target`).\n",
        "\n",
        "The `model` now holds this trained object, which can be used to make predictions on new, unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QeRoY7lyz1i"
      },
      "source": [
        "### 2. Linear Regression Model\n",
        "\n",
        "Linear regression assumes that the relationship between the features and the target vector is approximately linear. That is, the effect (also called coefficient, weight, or parameter) of the features on the target vector is constant. In the solution above, for the sake of explanation, the model was trained using only three features. Thus, the model is as follows:\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3 + \\varepsilon\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jodcvD5GS9mM"
      },
      "source": [
        "#### 2.1 Coefficient and Intecept\n",
        "\n",
        "In `sklearn`, the intercept $\\beta_0$ is stored in the model's `intercept_` attribute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "alTxed4t_ZL4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.float64(3.552713678800501e-15)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHz90_CWVrIx"
      },
      "source": [
        "Corospondingly, the coefficients (or weights) are stored in the model's `coef_` attribute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YpsfUyE8_ejT"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([44.19042807, 98.97517077, 58.15774073])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbJ2Hkux_mZv"
      },
      "source": [
        "#### 2.2 Make Predictions\n",
        "\n",
        "With the model in hand, we are able to predict the target when given a new sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hP5OzMV8_m7B"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction: -10.38, Target: -10.38, Error: 0.00\n",
            "Prediction: 25.51, Target: 25.51, Error: 0.00\n",
            "Prediction: 19.68, Target: 19.68, Error: 0.00\n",
            "Prediction: 149.50, Target: 149.50, Error: 0.00\n",
            "Prediction: -121.65, Target: -121.65, Error: 0.00\n",
            "Prediction: 90.29, Target: 90.29, Error: 0.00\n",
            "Prediction: 214.01, Target: 214.01, Error: 0.00\n",
            "Prediction: 224.74, Target: 224.74, Error: 0.00\n",
            "Prediction: -73.17, Target: -73.17, Error: 0.00\n",
            "Prediction: -195.63, Target: -195.63, Error: 0.00\n",
            "Prediction: -52.49, Target: -52.49, Error: 0.00\n",
            "Prediction: 201.80, Target: 201.80, Error: 0.00\n",
            "Prediction: 20.27, Target: 20.27, Error: 0.00\n",
            "Prediction: 89.16, Target: 89.16, Error: 0.00\n",
            "Prediction: -4.44, Target: -4.44, Error: 0.00\n",
            "Prediction: -45.48, Target: -45.48, Error: 0.00\n",
            "Prediction: 56.90, Target: 56.90, Error: 0.00\n",
            "Prediction: 120.55, Target: 120.55, Error: 0.00\n",
            "Prediction: -66.22, Target: -66.22, Error: 0.00\n",
            "Prediction: -43.84, Target: -43.84, Error: 0.00\n",
            "Prediction: 34.29, Target: 34.29, Error: 0.00\n",
            "Prediction: -77.51, Target: -77.51, Error: 0.00\n",
            "Prediction: -26.92, Target: -26.92, Error: 0.00\n",
            "Prediction: 197.20, Target: 197.20, Error: 0.00\n",
            "Prediction: 64.41, Target: 64.41, Error: 0.00\n",
            "Prediction: -58.24, Target: -58.24, Error: 0.00\n",
            "Prediction: -100.61, Target: -100.61, Error: 0.00\n",
            "Prediction: 120.31, Target: 120.31, Error: 0.00\n",
            "Prediction: -324.64, Target: -324.64, Error: 0.00\n",
            "Prediction: 11.80, Target: 11.80, Error: 0.00\n",
            "Prediction: -178.34, Target: -178.34, Error: 0.00\n",
            "Prediction: -12.12, Target: -12.12, Error: 0.00\n",
            "Prediction: -25.22, Target: -25.22, Error: 0.00\n",
            "Prediction: 162.27, Target: 162.27, Error: 0.00\n",
            "Prediction: 101.22, Target: 101.22, Error: 0.00\n",
            "Prediction: 128.54, Target: 128.54, Error: 0.00\n",
            "Prediction: -21.93, Target: -21.93, Error: 0.00\n",
            "Prediction: 141.56, Target: 141.56, Error: 0.00\n",
            "Prediction: 70.78, Target: 70.78, Error: 0.00\n",
            "Prediction: 42.76, Target: 42.76, Error: 0.00\n",
            "Prediction: -129.89, Target: -129.89, Error: 0.00\n",
            "Prediction: 75.63, Target: 75.63, Error: 0.00\n",
            "Prediction: 7.76, Target: 7.76, Error: 0.00\n",
            "Prediction: 27.83, Target: 27.83, Error: 0.00\n",
            "Prediction: -56.02, Target: -56.02, Error: 0.00\n",
            "Prediction: -45.13, Target: -45.13, Error: 0.00\n",
            "Prediction: -4.80, Target: -4.80, Error: 0.00\n",
            "Prediction: -224.88, Target: -224.88, Error: 0.00\n",
            "Prediction: -74.02, Target: -74.02, Error: 0.00\n",
            "Prediction: 32.48, Target: 32.48, Error: 0.00\n",
            "Prediction: -145.77, Target: -145.77, Error: 0.00\n",
            "Prediction: 70.44, Target: 70.44, Error: 0.00\n",
            "Prediction: 131.10, Target: 131.10, Error: 0.00\n",
            "Prediction: 152.75, Target: 152.75, Error: 0.00\n",
            "Prediction: -5.81, Target: -5.81, Error: 0.00\n",
            "Prediction: 126.04, Target: 126.04, Error: 0.00\n",
            "Prediction: -194.06, Target: -194.06, Error: 0.00\n",
            "Prediction: -16.07, Target: -16.07, Error: 0.00\n",
            "Prediction: -64.66, Target: -64.66, Error: 0.00\n",
            "Prediction: 89.27, Target: 89.27, Error: 0.00\n",
            "Prediction: 56.82, Target: 56.82, Error: 0.00\n",
            "Prediction: -116.59, Target: -116.59, Error: 0.00\n",
            "Prediction: 70.91, Target: 70.91, Error: 0.00\n",
            "Prediction: 56.13, Target: 56.13, Error: 0.00\n",
            "Prediction: 79.90, Target: 79.90, Error: 0.00\n",
            "Prediction: -44.90, Target: -44.90, Error: 0.00\n",
            "Prediction: -27.11, Target: -27.11, Error: 0.00\n",
            "Prediction: 100.82, Target: 100.82, Error: 0.00\n",
            "Prediction: 180.76, Target: 180.76, Error: 0.00\n",
            "Prediction: -45.03, Target: -45.03, Error: 0.00\n",
            "Prediction: -40.29, Target: -40.29, Error: 0.00\n",
            "Prediction: 4.71, Target: 4.71, Error: 0.00\n",
            "Prediction: -77.64, Target: -77.64, Error: 0.00\n",
            "Prediction: 17.78, Target: 17.78, Error: 0.00\n",
            "Prediction: 145.36, Target: 145.36, Error: 0.00\n",
            "Prediction: -32.14, Target: -32.14, Error: 0.00\n",
            "Prediction: 42.56, Target: 42.56, Error: 0.00\n",
            "Prediction: -50.69, Target: -50.69, Error: 0.00\n",
            "Prediction: 39.11, Target: 39.11, Error: 0.00\n",
            "Prediction: -11.17, Target: -11.17, Error: 0.00\n",
            "Prediction: 190.83, Target: 190.83, Error: 0.00\n",
            "Prediction: -41.25, Target: -41.25, Error: 0.00\n",
            "Prediction: -110.54, Target: -110.54, Error: 0.00\n",
            "Prediction: 108.40, Target: 108.40, Error: 0.00\n",
            "Prediction: 4.06, Target: 4.06, Error: 0.00\n",
            "Prediction: -11.23, Target: -11.23, Error: 0.00\n",
            "Prediction: 104.02, Target: 104.02, Error: 0.00\n",
            "Prediction: 135.56, Target: 135.56, Error: 0.00\n",
            "Prediction: 107.92, Target: 107.92, Error: 0.00\n",
            "Prediction: -17.17, Target: -17.17, Error: 0.00\n",
            "Prediction: 37.10, Target: 37.10, Error: 0.00\n",
            "Prediction: 63.82, Target: 63.82, Error: 0.00\n",
            "Prediction: -97.81, Target: -97.81, Error: 0.00\n",
            "Prediction: -145.52, Target: -145.52, Error: 0.00\n",
            "Prediction: 354.71, Target: 354.71, Error: 0.00\n",
            "Prediction: 26.61, Target: 26.61, Error: 0.00\n",
            "Prediction: -63.10, Target: -63.10, Error: 0.00\n",
            "Prediction: 112.40, Target: 112.40, Error: 0.00\n",
            "Prediction: 66.65, Target: 66.65, Error: 0.00\n",
            "Prediction: -18.76, Target: -18.76, Error: 0.00\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(features)\n",
        "for pred, tar in zip(predictions, target):\n",
        "    print(f\"Prediction: {pred:.2f}, Target: {tar:.2f}, Error: {abs(pred-tar):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvWqawlXagSo"
      },
      "source": [
        "#### 2.3 Evaluation using `score()`\n",
        "\n",
        "`sklearn` provides `score()` function for more efficient evaluation. The `model.score(X, y)` method performs two main steps internally:\n",
        "\n",
        "- It uses the input data `X` to generate predictions by calling the `model.predict(X)` method.\n",
        "\n",
        "- It then compares these predictions against the true labels `y` and returns a single score based on a default metric.\n",
        "\n",
        "The specific metric used depends on whether you are doing classification or regression.The function takes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9NW7ZFBaoo2"
      },
      "outputs": [],
      "source": [
        "model.score(features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZXky-jachUe"
      },
      "source": [
        "#### 2.4 Noise\n",
        "\n",
        "Data in the real world is rarely perfect and almost always contains noise. As a result, a model's predictions won't align perfectly with observed values. In fact, it's completely normal for a model to have some degree of error.\n",
        "\n",
        "The"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ia0RTilsdK4W"
      },
      "outputs": [],
      "source": [
        "# generate feature matrix, target vector\n",
        "features, target = make_regression(n_samples=100, n_features=3, n_informative=3, noise=5, random_state=1)\n",
        "\n",
        "# create linear regression\n",
        "regression = LinearRegression()\n",
        "\n",
        "# fit the linear regression\n",
        "model = regression.fit(features, target)\n",
        "model.score(features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwLniDuOd8oz"
      },
      "source": [
        "The make_regression function first generates data based on a perfect linear relationship：\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_3\n",
        "$$\n",
        "\n",
        "Here, vector $[\\beta_0, \\beta_1, \\beta_2, \\beta_3]$ represents the true coefficients (which can be obtained by setting the `coef` parameter to `True`). If the data were generated this way, all the points would fall perfectly on a straight line (or a hyperplane in higher dimensions).\n",
        "\n",
        "However, real-world data always has noise. The noise parameter is used to simulate this imperfection. It adds random numbers to the perfect y values, drawn from a normal distribution (also called a Gaussian distribution) with a mean of `0` and a standard deviation equal to the value you specify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64YCJ-sUpLHj"
      },
      "source": [
        "### 3. Interaction Effect\n",
        "\n",
        "An interaction effect (or simply \"interaction\") occurs when the effect of one independent variable on an outcome depends on the level or value of at least one other independent variable. In simpler terms, the variables don't work in isolation; their combined impact is different from the sum of their individual impacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PFRSkLlvz_F"
      },
      "source": [
        "#### 3.1 Simple Example\n",
        "\n",
        "For example, imagine a simple coffee-based example where we have two binary features--the presence of sugar  and whether or not we have stirred-—and we want to predict if the coffee tastes sweet. Just putting sugar in the coffee (`sugar=1, stirred=0`) won't make the coffee taste sweet (all the sugar is at the bottom!) and just stirring the coffee without adding sugar (`sugar=0, stirred=1`) won't make it sweet either. Instead it is the interaction of putting sugar in the coffee and stirring the coffee (`sugar=1, stirred=1`) that will make a coffee taste sweet. The effects of sugar and stirred on sweetness are dependent on each other. In this case we say there is an interaction effect between the features sugar and stirred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WURr7xZGqZFb"
      },
      "source": [
        "#### 3.2 Model the Interaction Effect\n",
        "\n",
        "Now, let's generate a group of samples with higher noise. We can certainly expect the model performs poorly on the new dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8xaS0a50qK0w"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9734073126511832"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# generate feature matrix, target vector\n",
        "features, target = make_regression(\n",
        "    n_samples=100, n_features=3, n_informative=3, noise=20, random_state=1)\n",
        "\n",
        "# create linear regression\n",
        "regression = LinearRegression()\n",
        "\n",
        "# fit the linear regression\n",
        "model = regression.fit(features, target)\n",
        "model.score(features, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGVBPDUrqs2S"
      },
      "source": [
        "Then we create the interaction term, as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZXdWPZerqykK"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "interaction = PolynomialFeatures(\n",
        "    degree=3, include_bias=False, interaction_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Era8qOcArGOm"
      },
      "source": [
        "The parameters of the interaction term are as follows:\n",
        "- `degree`\n",
        "This parameter defines the maximum degree of the features to be created. Here, \"degree\" refers to the total number of features multiplied together in a single term.\n",
        "\n",
        "- `interaction_only=False` (The default): This generates all feature combinations up to the specified degree, including powers of a single feature. For example, with $[x1, x2]$ and `degree=3`, it would generate $x_1$, $x_2$, $x_1^2$, $x_1\\cdot x_2$, $x_2^2$, $x_1^3$, $x_1^2\\cdot x_2$, $x_1\\cdot x_2^2$, $x_2^3$.\n",
        "- `interaction_only=True`: This generates only the products of different features (interaction terms) and will not generate powers of a single feature (like x1² or x1³). Its goal is purely to capture the \"interaction\" between features, not the non-linear relationship of a single feature with itself.\n",
        "- `include_bias=True` (The default): This adds a column of all 1s as the first feature. This 1 corresponds to the constant term (the intercept) in a polynomial equation, equivalent to x_1\n",
        "0\n",
        "- `include_bias=False`: This does not add the column of all 1s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qi6kUX4rEoB"
      },
      "source": [
        "The `.fit_transform(features)` method applies this rule to the original features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7hP91cOuqQv"
      },
      "outputs": [],
      "source": [
        "features_interaction = interaction.fit_transform(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsTIaZ_7u0_A"
      },
      "source": [
        "Finally, a standard `LinearRegression` model is created and trained using the `.fit()` method. Crucially, it's trained on the enhanced feature matrix, `features_interaction`, which includes the interaction term, allowing the model to find the best coefficients for not just $x_1$ and $x_2$, but also for their product, $x_1\\cdot x_2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d1L1EESu1T6"
      },
      "outputs": [],
      "source": [
        "regression = LinearRegression()\n",
        "\n",
        "# Fit the linear regression\n",
        "model = regression.fit(features_interaction, target)\n",
        "model.score(features_interaction, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN0FyRHqwFv1"
      },
      "source": [
        "### 4 Nonlinear Relationship\n",
        "\n",
        "Although linear relationships are highly interpretable and practical, nonlinear relationships are more prevalent in reality. Consequently, polynomial fitting often yields superior results. As the most widely used machine learning library, `scikit-learn` inherently supports polynomial fitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-yUnb4iy1TQ"
      },
      "source": [
        "#### 4.1 Creating Polynomial Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Umh9-0e-yr3T"
      },
      "outputs": [],
      "source": [
        "# Create polynomial features x^2 and x^3\n",
        "polynomial = PolynomialFeatures(degree=3, include_bias=False)\n",
        "features_polynomial = polynomial.fit_transform(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW_xUZK9y7Oi"
      },
      "source": [
        "#### 4.2 Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt59OKqMy8IU"
      },
      "outputs": [],
      "source": [
        "# Create linear regression\n",
        "regression = LinearRegression()\n",
        "\n",
        "# Fit the linear regression\n",
        "model = regression.fit(features_polynomial, target)\n",
        "model.score(features_polynomial, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwGBJ8nnzkAZ"
      },
      "source": [
        "### 5. Standardization, Normalization, and Regularization\n",
        "\n",
        "Standardization, Normalization, and Regularization are three concepts in machine learning that are often easily confused. In this context, we will briefly discuss regularization through the lens of the linear regression problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWXqp9lHBY6t"
      },
      "source": [
        "#### 5.1 Standardization\n",
        "\n",
        "`sklearn` provides strong support for data preprocessing. For standardization, the `StandardScaler` is quite convinient. First, let's generate the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOBF4B74fZEw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "features, target = make_regression(n_samples = 100,\n",
        "                                   n_features = 3,\n",
        "                                   n_informative = 2,\n",
        "                                   n_targets = 1,\n",
        "                                   noise = 0.2,\n",
        "                                   coef = False,\n",
        "                                   random_state = 1)\n",
        "\n",
        "print(np.mean(features, axis=0), np.var(features, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTmlz7dohRat"
      },
      "source": [
        "Then, we can perform standarization on the generated data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcGqPRjkfkQW"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_standardized = scaler.fit_transform(features)\n",
        "print(np.mean(features_standardized, axis=0), np.var(features_standardized, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFrVDouXhaP3"
      },
      "source": [
        "You will notice that after standarization, the variables in the data follows normal distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgA1P6t8hl-C"
      },
      "source": [
        "#### 5.2 Ridge Regression\n",
        "\n",
        "The idea of regularization is to add a penalty to the original loss function. For a brief understanding, we present the loss function of linear regression.  The standard loss function for linear regression is the Mean Squared Error (MSE). The loss reflects the average squared difference between the model's predictions and the actual data.\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2n}Σ_{i=1}^n(y_i -\\hat{y_i})^2\n",
        "$$\n",
        "\n",
        "If you don't take the average of the loss, you will get the residual sum of squares (RSS).\n",
        "$$\n",
        "RSS = Σ_{i=1}^n(y_i -\\hat{y_i})^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gri9qg0mj09C"
      },
      "source": [
        "The `ridge` regression adds a penalty that is is a tuning hyperparameter multiplied by the squared sum of all coefficients to the loss function, as follows:\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2n}Σ_{i=1}^n(y_i -\\hat{y_i})^2 + \\lambda\\Sigma_{j=1}^n\\hat{\\beta_j}^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-UPFW_-zBVX"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "regression = Ridge(alpha=0.5)\n",
        "model_ridge = regression.fit(features_standardized, target)\n",
        "model_ridge.score(features_standardized, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfB8TFYpzXen"
      },
      "source": [
        "#### 5.3 Lasso Regression\n",
        "\n",
        "Lasso regression, on the other hand, add L1 regularization to the loss, which lead to:\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{2n}Σ_{i=1}^n(y_i -\\hat{y_i})^2 + \\lambda\\Sigma_{j=1}^n|\\hat{\\beta_j}|\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFyT5Hu_8fKR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "regression = Lasso(alpha=0.5)\n",
        "model_lasso = regression.fit(features_standardized, target)\n",
        "model_lasso.score(features_standardized, target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pts94Bj_8orU"
      },
      "source": [
        "Ridge (L2 regularization) and Lasso (L1 regularization) yield distinct outcomes. Intuitively, ridge regression does not force coefficients to be exactly zero, but rather shrinks the magnitude of each coefficient. In contrast, lasso regression performs a function analogous to feature selection by setting the coefficients of irrelevant features to zero."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFsI8_nG97zZ"
      },
      "outputs": [],
      "source": [
        "print(model_ridge.coef_)\n",
        "print(model_lasso.coef_)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
